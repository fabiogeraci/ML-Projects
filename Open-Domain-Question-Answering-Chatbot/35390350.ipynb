{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Pritam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from bert_serving.client import BertClient\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from nltk.metrics import masi_distance\n",
    "import string\n",
    "import textdistance\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim.downloader as api\n",
    "glove_300 = api.load(\"glove-wiki-gigaword-300\")\n",
    "#glove_300.save_word2vec_format('glove.6B.300d.txt')\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "nltk.download('punkt')\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "#Normalizing sentence vectors with TFIDF\n",
    "def tfidf(question,context):\n",
    "    concat=[]\n",
    "    concat.append(question)\n",
    "    concat.append(context)\n",
    "    #print(concat)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(concat)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    #print(vals)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    #print(req_tfidf)\n",
    "    return req_tfidf\n",
    "#Function for Jaccard similarity\n",
    "def jaccard_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union\n",
    "#datasize=250\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "question_words=[\"what\",\"which\",\"who\",\"whom\",\"whoose\",\"when\",\"where\",\"whence\",\"why\",\"how\",\"?\"]\n",
    "df=pd.read_csv('WikiQA-train.tsv',sep='\\t')\n",
    "#print(Counter(df.iloc[:,-1]))\n",
    "test1=df[df['Label']==1]\n",
    "test1=test1.iloc[:1000]\n",
    "test2=df[df['Label']==0]\n",
    "test2=test2.iloc[:1000]\n",
    "#print(test2)\n",
    "df=pd.concat([test1,test2],axis=0)\n",
    "#test.to_csv('C:\\\\Users\\\\Pritam\\\\Desktop\\\\WikiQACorpus\\\\test.csv')\n",
    "#df=df.iloc[:500]\n",
    "#x_train, x_test, y_train, y_test = train_test_split( df.iloc[:,:-1], df.iloc[:,-1], test_size=0.2, random_state=42)\n",
    "result=list()\n",
    "qid=''\n",
    "temp=list()\n",
    "pred=list()\n",
    "cosine=[]\n",
    "lav_dis=[]\n",
    "jac_sim=[]\n",
    "mscore=[]\n",
    "ham_dis=[]\n",
    "jwscore=[]\n",
    "sdscore=[]\n",
    "roscore=[]\n",
    "escore=[]\n",
    "wmd=[]\n",
    "ner_feature=[]\n",
    "common_keywords=[]\n",
    "count_noun_chunks=[]\n",
    "tfidf_cosine=[]\n",
    "for row in range(0,len(df)-1):\n",
    "    #j=tokenise()\n",
    "    #print(row)\n",
    "    #concat_list=[]\n",
    "    if row==0:\n",
    "        qid=df.iloc[row]['QuestionID']\n",
    "\n",
    "    #t=list()\n",
    "    ques=df.iloc[row]['Question'].lower() #obtaining data from the dataset row wise\n",
    "    my_doc = nlp(df.iloc[row]['Question'].lower())\n",
    "    context=df.iloc[row]['Sentence'].lower()\n",
    "    #concat_list.append(ques)\n",
    "    #concat_list.append(context)\n",
    "\n",
    "    token_list = []    #Creating  token list\n",
    "    for token in my_doc:\n",
    "        token_list.append(token.text) \n",
    "    sen=\"\"\n",
    "    for word in token_list:  #Removing question words and symbol and reformulating\n",
    "        if word.lower() not in question_words:\n",
    "            sen=sen+word+\" \"\n",
    "    ques=sen\n",
    "    tfidf_cos=tfidf(ques,context) #Measuring various distance based metrics\n",
    "    tfidf_cosine.append(tfidf_cos)\n",
    "    q_ner=list()\n",
    "    doc1=nlp(sen)\n",
    "    for ent in doc1.ents:    #obtaining NER from question\n",
    "        q_ner.append(ent.text)\n",
    "    \n",
    "    doc2=nlp(context)\n",
    "    ner_count=0\n",
    "    for ent in doc2.ents:     #obtaining NER from context\n",
    "            ner_count=ner_count+1\n",
    "    ner_feature.append(ner_count)\n",
    "    keycount=0\n",
    "    rootq = [token for token in doc1 if token.head == token][0]  #Root word in dependency parsing of the question\n",
    "    #only taking noun/verb/proper noun as important key words\n",
    "    keywordsq=[t.text for t in rootq.subtree if t.pos_==\"NOUN\" or t.pos_==\"PROPN\" or t.pos_==\"VERB\" or t.pos_==\"NUM\"]\n",
    "    rootc = [token for token in doc2 if token.head == token][0]  #Root word of the context\n",
    "    keywordsc=[t.text for t in rootc.subtree if t.pos_==\"NOUN\" or t.pos_==\"PROPN\" or t.pos_==\"VERB\" or t.pos_==\"NUM\"]\n",
    "    for key in keywordsq:\n",
    "        if key in keywordsc:\n",
    "            keycount=keycount+1\n",
    "    common_keywords.append(keycount)\n",
    "    #ner_feature.append(ner_count)\n",
    "    q_noun_chunks=list(doc1.noun_chunks)  #noun chunks in sentences\n",
    "    c_noun_chunks=list(doc2.noun_chunks)\n",
    "    count_chunks=0\n",
    "    for chunk in c_noun_chunks:    #validate common noun chunks\n",
    "        if chunk in q_noun_chunks:\n",
    "            count_chunks=count_chunks+1\n",
    "    count_noun_chunks.append(count_chunks)\n",
    "    bc = BertClient(check_length=False)  #Bert client object\n",
    "    question_vec = bc.encode([sen])[0]  #encoding with bert as service\n",
    "    c_vec=bc.encode([context])\n",
    "    score=np.sum(question_vec * c_vec, axis=1) / np.linalg.norm(c_vec, axis=1)  #cosine similarity\n",
    "    cosine.append(score[0])\n",
    "    #cosine.append(0)\n",
    "    temp=glove_300.wmdistance(ques,context)   #Word Movers Distance on GLOVE 300\n",
    "    if np.isnan(temp)==False:\n",
    "        temp=0.0\n",
    "    wmd.append(temp)   \n",
    "    lav_dis.append(nltk.edit_distance(ques,context)) #Lavenstein Distance\n",
    "    jac_sim.append(jaccard_similarity(ques.split(),context.split())) #Jaccard\n",
    "    #jscore=jaccard_similarity(question_vec,c_vec.flatten())\n",
    "    mscore.append(masi_distance(set(ques.split()),set(context.split()))) #MASI\n",
    "    ham_dis.append(textdistance.hamming(ques, context)) #HAMMING Distance\n",
    "    jwscore.append(textdistance.jaro_winkler(ques,context)) #Jaro-winkler distance\n",
    "    #Sorensen-Dice\n",
    "    sdscore.append(textdistance.sorensen(ques.split() , context.split())) #Sorensen-Dice\n",
    "    #Ratcliff-Obershelp similarity\n",
    "    roscore.append(textdistance.ratcliff_obershelp(ques,context)) #Ratcliff-Obershelp_similarity\n",
    "    escore.append(textdistance.entropy_ncd(ques,context))  #entropy\n",
    "    \n",
    "    #print(c_vec.flatten().shape)\n",
    "    #print(question_vec.shape)\n",
    "    #print(cosine)\n",
    "    \n",
    "#print(len(lav_dis))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Cosine_Similarity  Levenshtein_distance  Jaccard_similarity  \\\n",
      "0             10.497584                    40            0.125000   \n",
      "1              9.606836                    31            0.153846   \n",
      "2              9.913586                   125            0.103448   \n",
      "3              9.524587                    38            0.153846   \n",
      "4              7.973140                    82            0.250000   \n",
      "5              8.701014                   114            0.120000   \n",
      "6              9.504800                   149            0.000000   \n",
      "7              6.690885                   130            0.076923   \n",
      "8              9.593227                   147            0.032258   \n",
      "9             11.005894                    83            0.000000   \n",
      "10             8.428024                   187            0.075000   \n",
      "11             7.642433                   185            0.025000   \n",
      "12             7.836153                   134            0.000000   \n",
      "13             5.715432                    69            0.000000   \n",
      "14             7.558857                   100            0.125000   \n",
      "15             7.321880                    97            0.083333   \n",
      "16             7.601317                   156            0.111111   \n",
      "17             9.349541                   149            0.121212   \n",
      "18            11.820915                   181            0.076923   \n",
      "19             8.926131                   165            0.030303   \n",
      "20             7.102811                   121            0.040000   \n",
      "21             7.502049                   190            0.052632   \n",
      "22             5.918828                   108            0.000000   \n",
      "23            11.316751                    99            0.181818   \n",
      "24            11.576438                    56            0.105263   \n",
      "25             5.045810                    79            0.050000   \n",
      "26            10.481433                    89            0.095238   \n",
      "27             7.417614                   103            0.000000   \n",
      "28            11.128270                    50            0.142857   \n",
      "29             9.691464                   211            0.047619   \n",
      "...                 ...                   ...                 ...   \n",
      "1969           7.160292                   130            0.031250   \n",
      "1970           7.762640                    83            0.045455   \n",
      "1971           7.502476                    60            0.000000   \n",
      "1972           6.417366                    46            0.000000   \n",
      "1973           7.009513                    69            0.125000   \n",
      "1974           7.223442                    79            0.062500   \n",
      "1975           6.989480                    98            0.080000   \n",
      "1976           7.304649                   214            0.047619   \n",
      "1977           8.538459                   163            0.030303   \n",
      "1978           8.157218                   210            0.000000   \n",
      "1979           8.129889                   102            0.037037   \n",
      "1980           8.140699                    93            0.041667   \n",
      "1981           7.011475                    91            0.045455   \n",
      "1982           8.030614                   128            0.000000   \n",
      "1983           7.466805                   115            0.000000   \n",
      "1984           7.431872                   162            0.000000   \n",
      "1985           8.286135                   122            0.000000   \n",
      "1986           8.194794                   206            0.024390   \n",
      "1987           7.698233                   133            0.031250   \n",
      "1988           8.295203                    90            0.000000   \n",
      "1989           8.265609                    61            0.055556   \n",
      "1990           6.723648                   140            0.000000   \n",
      "1991           7.428019                   124            0.031250   \n",
      "1992           7.513961                    69            0.045455   \n",
      "1993           8.419636                   132            0.031250   \n",
      "1994           7.939460                   157            0.000000   \n",
      "1995           6.507997                    34            0.000000   \n",
      "1996           7.013045                   158            0.027027   \n",
      "1997           6.809761                    43            0.000000   \n",
      "1998           6.598661                    68            0.052632   \n",
      "\n",
      "      Masi_distance  Hamming_dustance  JaroWinkler_socre  Sorensen-Dice  \\\n",
      "0          0.945000                57           0.638648       0.222222   \n",
      "1          0.949231                44           0.635024       0.266667   \n",
      "2          0.963333               149           0.559513       0.187500   \n",
      "3          0.949231                55           0.597024       0.266667   \n",
      "4          0.913158               111           0.625477       0.400000   \n",
      "5          0.956957               116           0.687881       0.214286   \n",
      "6          1.000000               179           0.601943       0.000000   \n",
      "7          0.973600               142           0.513529       0.142857   \n",
      "8          0.987308               170           0.575315       0.062500   \n",
      "9          1.000000               101           0.636957       0.000000   \n",
      "10         0.974615               203           0.550022       0.139535   \n",
      "11         0.991081               210           0.546032       0.048780   \n",
      "12         1.000000               155           0.514888       0.000000   \n",
      "13         1.000000                83           0.528171       0.000000   \n",
      "14         0.955000               120           0.593363       0.222222   \n",
      "15         0.971304               115           0.570134       0.153846   \n",
      "16         0.957419               185           0.571345       0.200000   \n",
      "17         0.956000               176           0.578742       0.216216   \n",
      "18         0.957419               217           0.574264       0.142857   \n",
      "19         0.989000               183           0.524314       0.058824   \n",
      "20         0.986800               137           0.522109       0.076923   \n",
      "21         0.978710               210           0.552787       0.100000   \n",
      "22         1.000000               122           0.552764       0.000000   \n",
      "23         0.937143               130           0.655860       0.307692   \n",
      "24         0.963333                93           0.726956       0.190476   \n",
      "25         0.983500                97           0.526361       0.095238   \n",
      "26         0.968571               111           0.592262       0.173913   \n",
      "27         1.000000               123           0.538157       0.000000   \n",
      "28         0.952857                63           0.580366       0.250000   \n",
      "29         0.982162               251           0.554925       0.090909   \n",
      "...             ...               ...                ...            ...   \n",
      "1969       0.988621               150           0.534331       0.060606   \n",
      "1970       0.985000                99           0.600179       0.086957   \n",
      "1971       1.000000                75           0.609783       0.000000   \n",
      "1972       1.000000                62           0.651006       0.000000   \n",
      "1973       0.958750                89           0.605914       0.222222   \n",
      "1974       0.979375                95           0.600123       0.117647   \n",
      "1975       0.970000               117           0.582826       0.148148   \n",
      "1976       0.983500               244           0.557341       0.090909   \n",
      "1977       0.989688               190           0.539937       0.058824   \n",
      "1978       1.000000               235           0.534406       0.000000   \n",
      "1979       0.986800               123           0.581134       0.071429   \n",
      "1980       0.986250               117           0.588310       0.080000   \n",
      "1981       0.985000               108           0.539514       0.086957   \n",
      "1982       1.000000               150           0.543555       0.000000   \n",
      "1983       1.000000               133           0.501362       0.000000   \n",
      "1984       1.000000               182           0.536745       0.000000   \n",
      "1985       1.000000               141           0.525303       0.000000   \n",
      "1986       0.991316               230           0.544620       0.047619   \n",
      "1987       0.989355               151           0.568837       0.060606   \n",
      "1988       1.000000               108           0.523176       0.000000   \n",
      "1989       0.980588                75           0.613064       0.105263   \n",
      "1990       1.000000               158           0.549640       0.000000   \n",
      "1991       0.989000               140           0.546271       0.060606   \n",
      "1992       0.984286                82           0.577646       0.086957   \n",
      "1993       0.987778               158           0.578402       0.060606   \n",
      "1994       1.000000               181           0.536732       0.000000   \n",
      "1995       1.000000                40           0.511275       0.000000   \n",
      "1996       0.989000               178           0.521285       0.052632   \n",
      "1997       1.000000                51           0.584831       0.000000   \n",
      "1998       0.981667                84           0.583987       0.100000   \n",
      "\n",
      "      Ratcliff-Obershelp_similarity  NCD_entropy  Common_NER  \\\n",
      "0                          0.534884     0.045509           0   \n",
      "1                          0.552632     0.018784           1   \n",
      "2                          0.245810     0.056882           3   \n",
      "3                          0.395349     0.032083           2   \n",
      "4                          0.421053     0.111003           1   \n",
      "5                          0.269231     0.124304           0   \n",
      "6                          0.181818     0.071542           0   \n",
      "7                          0.181818     0.229890           1   \n",
      "8                          0.222222     0.090381           0   \n",
      "9                          0.328767     0.092626           1   \n",
      "10                         0.217054     0.066366           4   \n",
      "11                         0.191667     0.123808           2   \n",
      "12                         0.233333     0.118415           5   \n",
      "13                         0.220183     0.139454           3   \n",
      "14                         0.313725     0.110125           2   \n",
      "15                         0.263889     0.155909           0   \n",
      "16                         0.236364     0.116914           4   \n",
      "17                         0.287037     0.149570           4   \n",
      "18                         0.259259     0.077054           8   \n",
      "19                         0.122642     0.093790           4   \n",
      "20                         0.175000     0.129137           4   \n",
      "21                         0.200000     0.064833           0   \n",
      "22                         0.186667     0.061285           0   \n",
      "23                         0.494949     0.060431           0   \n",
      "24                         0.559006     0.049531           0   \n",
      "25                         0.325203     0.194152           1   \n",
      "26                         0.311688     0.065168           3   \n",
      "27                         0.263158     0.132658           1   \n",
      "28                         0.176471     0.102967           1   \n",
      "29                         0.243421     0.043168           0   \n",
      "...                             ...          ...         ...   \n",
      "1969                       0.250000     0.112426           5   \n",
      "1970                       0.325926     0.075985           0   \n",
      "1971                       0.296296     0.091714           2   \n",
      "1972                       0.458333     0.063949           0   \n",
      "1973                       0.363636     0.085912           0   \n",
      "1974                       0.248062     0.080654           0   \n",
      "1975                       0.230769     0.110812           2   \n",
      "1976                       0.207143     0.125605           4   \n",
      "1977                       0.203540     0.150917           4   \n",
      "1978                       0.117647     0.130578           7   \n",
      "1979                       0.262500     0.098861           0   \n",
      "1980                       0.243590     0.110324           1   \n",
      "1981                       0.136054     0.123516           2   \n",
      "1982                       0.180851     0.118565           2   \n",
      "1983                       0.197674     0.107684           1   \n",
      "1984                       0.081818     0.119287           3   \n",
      "1985                       0.222222     0.108743           3   \n",
      "1986                       0.066667     0.118689           1   \n",
      "1987                       0.187500     0.104988           3   \n",
      "1988                       0.123288     0.120960           2   \n",
      "1989                       0.175439     0.097643           2   \n",
      "1990                       0.131313     0.106024           2   \n",
      "1991                       0.197802     0.103745           3   \n",
      "1992                       0.231405     0.125896           3   \n",
      "1993                       0.174359     0.102726           5   \n",
      "1994                       0.199095     0.100017           1   \n",
      "1995                       0.297297     0.082114           0   \n",
      "1996                       0.166667     0.064084           0   \n",
      "1997                       0.272727     0.050210           0   \n",
      "1998                       0.268908     0.060220           1   \n",
      "\n",
      "      Common_noun_chunks  Common_Keywords  Word-Movers Distance  \\\n",
      "0                      0                2                   0.0   \n",
      "1                      0                1                   0.0   \n",
      "2                      0                2                   0.0   \n",
      "3                      0                2                   0.0   \n",
      "4                      0                2                   0.0   \n",
      "5                      0                2                   0.0   \n",
      "6                      0                0                   0.0   \n",
      "7                      0                1                   0.0   \n",
      "8                      0                0                   0.0   \n",
      "9                      0                0                   0.0   \n",
      "10                     0                1                   0.0   \n",
      "11                     0                1                   0.0   \n",
      "12                     0                1                   0.0   \n",
      "13                     0                0                   0.0   \n",
      "14                     0                4                   0.0   \n",
      "15                     0                3                   0.0   \n",
      "16                     0                4                   0.0   \n",
      "17                     0                3                   0.0   \n",
      "18                     0                2                   0.0   \n",
      "19                     0                0                   0.0   \n",
      "20                     0                1                   0.0   \n",
      "21                     0                1                   0.0   \n",
      "22                     0                1                   0.0   \n",
      "23                     0                2                   0.0   \n",
      "24                     0                1                   0.0   \n",
      "25                     0                2                   0.0   \n",
      "26                     0                1                   0.0   \n",
      "27                     0                0                   0.0   \n",
      "28                     0                3                   0.0   \n",
      "29                     0                1                   0.0   \n",
      "...                  ...              ...                   ...   \n",
      "1969                   0                0                   0.0   \n",
      "1970                   0                0                   0.0   \n",
      "1971                   0                0                   0.0   \n",
      "1972                   0                0                   0.0   \n",
      "1973                   0                1                   0.0   \n",
      "1974                   0                1                   0.0   \n",
      "1975                   0                1                   0.0   \n",
      "1976                   0                2                   0.0   \n",
      "1977                   0                1                   0.0   \n",
      "1978                   0                0                   0.0   \n",
      "1979                   0                1                   0.0   \n",
      "1980                   0                1                   0.0   \n",
      "1981                   0                1                   0.0   \n",
      "1982                   0                0                   0.0   \n",
      "1983                   0                0                   0.0   \n",
      "1984                   0                0                   0.0   \n",
      "1985                   0                0                   0.0   \n",
      "1986                   0                1                   0.0   \n",
      "1987                   0                1                   0.0   \n",
      "1988                   0                0                   0.0   \n",
      "1989                   0                1                   0.0   \n",
      "1990                   0                0                   0.0   \n",
      "1991                   0                1                   0.0   \n",
      "1992                   0                1                   0.0   \n",
      "1993                   0                1                   0.0   \n",
      "1994                   0                0                   0.0   \n",
      "1995                   0                0                   0.0   \n",
      "1996                   0                1                   0.0   \n",
      "1997                   0                0                   0.0   \n",
      "1998                   0                1                   0.0   \n",
      "\n",
      "      Tfidf_Cosine_Similarity  Label  \n",
      "0                    0.871366      1  \n",
      "1                    0.150640      1  \n",
      "2                    0.133284      1  \n",
      "3                    0.136276      1  \n",
      "4                    0.295059      1  \n",
      "5                    0.356300      1  \n",
      "6                    0.000000      1  \n",
      "7                    0.168310      1  \n",
      "8                    0.222032      1  \n",
      "9                    0.000000      1  \n",
      "10                   0.110267      1  \n",
      "11                   0.053997      1  \n",
      "12                   0.000000      1  \n",
      "13                   0.000000      1  \n",
      "14                   0.242919      1  \n",
      "15                   0.168310      1  \n",
      "16                   0.329204      1  \n",
      "17                   0.189435      1  \n",
      "18                   0.439404      1  \n",
      "19                   0.104751      1  \n",
      "20                   0.076428      1  \n",
      "21                   0.383862      1  \n",
      "22                   0.116718      1  \n",
      "23                   0.245584      1  \n",
      "24                   0.194314      1  \n",
      "25                   0.168310      1  \n",
      "26                   0.098656      1  \n",
      "27                   0.000000      1  \n",
      "28                   0.291219      1  \n",
      "29                   0.156297      1  \n",
      "...                       ...    ...  \n",
      "1969                 0.121603      0  \n",
      "1970                 0.335176      0  \n",
      "1971                 0.000000      0  \n",
      "1972                 0.220288      0  \n",
      "1973                 0.161714      0  \n",
      "1974                 0.311257      0  \n",
      "1975                 0.168310      0  \n",
      "1976                 0.173174      0  \n",
      "1977                 0.056989      0  \n",
      "1978                 0.000000      0  \n",
      "1979                 0.079703      0  \n",
      "1980                 0.087687      0  \n",
      "1981                 0.079703      0  \n",
      "1982                 0.000000      0  \n",
      "1983                 0.000000      0  \n",
      "1984                 0.000000      0  \n",
      "1985                 0.000000      0  \n",
      "1986                 0.052513      0  \n",
      "1987                 0.070984      0  \n",
      "1988                 0.000000      0  \n",
      "1989                 0.098680      0  \n",
      "1990                 0.000000      0  \n",
      "1991                 0.076450      0  \n",
      "1992                 0.079703      0  \n",
      "1993                 0.044146      0  \n",
      "1994                 0.000000      0  \n",
      "1995                 0.000000      0  \n",
      "1996                 0.060562      0  \n",
      "1997                 0.000000      0  \n",
      "1998                 0.168368      0  \n",
      "\n",
      "[1999 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "featureset = pd.DataFrame(list(zip(cosine,lav_dis,jac_sim,mscore,ham_dis,jwscore,sdscore,roscore,escore,ner_feature,count_noun_chunks,common_keywords,wmd,tfidf_cosine,df.iloc[:,-1])), \n",
    "               columns =[\"Cosine_Similarity\",\"Levenshtein_distance\",\"Jaccard_similarity\",\"Masi_distance\",\"Hamming_dustance\",\"JaroWinkler_socre\",\"Sorensen-Dice\",\"Ratcliff-Obershelp_similarity\",\"NCD_entropy\",\"Common_NER\",\"Common_noun_chunks\",\"Common_Keywords\",\"Word-Movers Distance\",\"Tfidf_Cosine_Similarity\",\"Label\"])\n",
    "print(featureset)\n",
    "#construction of featureset\n",
    "featureset.to_csv('C:\\\\Users\\\\Pritam\\\\Desktop\\\\WikiQACorpus\\\\features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=0, shuffle=True),\n",
       "       error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('standardize', StandardScaler(copy=True, with_mean=True, with_std=True)), ('Normalizer', Normalizer(copy=True, norm='l2')), ('clf', None)]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'clf': [RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_w...\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1)]},\n",
       "       pre_dispatch='2*n_jobs', refit='f1_weighted',\n",
       "       return_train_score=False,\n",
       "       scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'],\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "x_train, x_test, y_train, y_test = train_test_split( featureset.iloc[:,:-1], featureset.iloc[:,-1], test_size=0.2, random_state=42)\n",
    "model = Pipeline([\n",
    "    ('standardize', StandardScaler()),             #Model Pipeline- self explanatory\n",
    "    #('MinMax', MinMaxScaler()),\n",
    "    #('Robust Scalar', RobustScaler()) ,\n",
    "    ('Normalizer', Normalizer()),\n",
    "    #('norm', TfidfTransformer(norm=None)),\n",
    "    ('clf', None), #clf set in param_grid.\n",
    "])\n",
    "\n",
    "param_grid={\n",
    "                'clf': [RandomForestClassifier(), LogisticRegression(solver='liblinear', random_state=0), SVC(probability=True),tree.DecisionTreeClassifier(),GradientBoostingClassifier(),XGBClassifier()],\n",
    "                      }\n",
    "search = GridSearchCV(model, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0), \n",
    "                      return_train_score=False, \n",
    "                      scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'],\n",
    "                      refit = 'f1_weighted',\n",
    "                      param_grid=param_grid\n",
    "                      )\n",
    "search.fit(x_train, y_train)  #Fitting all models with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_accuracy</th>\n",
       "      <th>split1_test_accuracy</th>\n",
       "      <th>split2_test_accuracy</th>\n",
       "      <th>split3_test_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>std_test_recall_weighted</th>\n",
       "      <th>rank_test_recall_weighted</th>\n",
       "      <th>split0_test_f1_weighted</th>\n",
       "      <th>split1_test_f1_weighted</th>\n",
       "      <th>split2_test_f1_weighted</th>\n",
       "      <th>split3_test_f1_weighted</th>\n",
       "      <th>split4_test_f1_weighted</th>\n",
       "      <th>mean_test_f1_weighted</th>\n",
       "      <th>std_test_f1_weighted</th>\n",
       "      <th>rank_test_f1_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.026329</td>\n",
       "      <td>0.001739</td>\n",
       "      <td>0.006184</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'clf': (DecisionTreeClassifier(class_weight=N...</td>\n",
       "      <td>0.660436</td>\n",
       "      <td>0.690625</td>\n",
       "      <td>0.659375</td>\n",
       "      <td>0.639498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017343</td>\n",
       "      <td>5</td>\n",
       "      <td>0.657743</td>\n",
       "      <td>0.689646</td>\n",
       "      <td>0.658725</td>\n",
       "      <td>0.637703</td>\n",
       "      <td>0.672345</td>\n",
       "      <td>0.663239</td>\n",
       "      <td>0.017214</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004987</td>\n",
       "      <td>0.003025</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'clf': LogisticRegression(C=1.0, class_weight...</td>\n",
       "      <td>0.691589</td>\n",
       "      <td>0.746875</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.686520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022378</td>\n",
       "      <td>2</td>\n",
       "      <td>0.691427</td>\n",
       "      <td>0.746788</td>\n",
       "      <td>0.718728</td>\n",
       "      <td>0.686490</td>\n",
       "      <td>0.695931</td>\n",
       "      <td>0.707883</td>\n",
       "      <td>0.022375</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.174733</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>0.027128</td>\n",
       "      <td>0.001934</td>\n",
       "      <td>SVC(C=1.0, cache_size=200, class_weight=None, ...</td>\n",
       "      <td>{'clf': SVC(C=1.0, cache_size=200, class_weigh...</td>\n",
       "      <td>0.688474</td>\n",
       "      <td>0.753125</td>\n",
       "      <td>0.721875</td>\n",
       "      <td>0.677116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026636</td>\n",
       "      <td>1</td>\n",
       "      <td>0.688419</td>\n",
       "      <td>0.752780</td>\n",
       "      <td>0.721704</td>\n",
       "      <td>0.676773</td>\n",
       "      <td>0.711349</td>\n",
       "      <td>0.710211</td>\n",
       "      <td>0.026603</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.012966</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.002593</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'clf': DecisionTreeClassifier(class_weight=No...</td>\n",
       "      <td>0.619938</td>\n",
       "      <td>0.628125</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017010</td>\n",
       "      <td>6</td>\n",
       "      <td>0.619923</td>\n",
       "      <td>0.628129</td>\n",
       "      <td>0.593432</td>\n",
       "      <td>0.586191</td>\n",
       "      <td>0.623787</td>\n",
       "      <td>0.610305</td>\n",
       "      <td>0.017070</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.128257</td>\n",
       "      <td>0.003816</td>\n",
       "      <td>0.004389</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>{'clf': ([DecisionTreeRegressor(criterion='fri...</td>\n",
       "      <td>0.669782</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.671875</td>\n",
       "      <td>0.639498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030920</td>\n",
       "      <td>3</td>\n",
       "      <td>0.669500</td>\n",
       "      <td>0.734118</td>\n",
       "      <td>0.671878</td>\n",
       "      <td>0.636999</td>\n",
       "      <td>0.672500</td>\n",
       "      <td>0.677022</td>\n",
       "      <td>0.031507</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.115292</td>\n",
       "      <td>0.004746</td>\n",
       "      <td>0.006981</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'clf': XGBClassifier(base_score=0.5, booster=...</td>\n",
       "      <td>0.644860</td>\n",
       "      <td>0.740625</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.639498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036057</td>\n",
       "      <td>4</td>\n",
       "      <td>0.644037</td>\n",
       "      <td>0.740262</td>\n",
       "      <td>0.674924</td>\n",
       "      <td>0.636603</td>\n",
       "      <td>0.678798</td>\n",
       "      <td>0.674927</td>\n",
       "      <td>0.036631</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.026329      0.001739         0.006184        0.000399   \n",
       "1       0.004987      0.003025         0.002792        0.000747   \n",
       "2       0.174733      0.007939         0.027128        0.001934   \n",
       "3       0.012966      0.000631         0.002593        0.000488   \n",
       "4       0.128257      0.003816         0.004389        0.000488   \n",
       "5       0.115292      0.004746         0.006981        0.000631   \n",
       "\n",
       "                                           param_clf  \\\n",
       "0  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "1  LogisticRegression(C=1.0, class_weight=None, d...   \n",
       "2  SVC(C=1.0, cache_size=200, class_weight=None, ...   \n",
       "3  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "4  ([DecisionTreeRegressor(criterion='friedman_ms...   \n",
       "5  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
       "\n",
       "                                              params  split0_test_accuracy  \\\n",
       "0  {'clf': (DecisionTreeClassifier(class_weight=N...              0.660436   \n",
       "1  {'clf': LogisticRegression(C=1.0, class_weight...              0.691589   \n",
       "2  {'clf': SVC(C=1.0, cache_size=200, class_weigh...              0.688474   \n",
       "3  {'clf': DecisionTreeClassifier(class_weight=No...              0.619938   \n",
       "4  {'clf': ([DecisionTreeRegressor(criterion='fri...              0.669782   \n",
       "5  {'clf': XGBClassifier(base_score=0.5, booster=...              0.644860   \n",
       "\n",
       "   split1_test_accuracy  split2_test_accuracy  split3_test_accuracy  \\\n",
       "0              0.690625              0.659375              0.639498   \n",
       "1              0.746875              0.718750              0.686520   \n",
       "2              0.753125              0.721875              0.677116   \n",
       "3              0.628125              0.593750              0.586207   \n",
       "4              0.734375              0.671875              0.639498   \n",
       "5              0.740625              0.675000              0.639498   \n",
       "\n",
       "           ...            std_test_recall_weighted  rank_test_recall_weighted  \\\n",
       "0          ...                            0.017343                          5   \n",
       "1          ...                            0.022378                          2   \n",
       "2          ...                            0.026636                          1   \n",
       "3          ...                            0.017010                          6   \n",
       "4          ...                            0.030920                          3   \n",
       "5          ...                            0.036057                          4   \n",
       "\n",
       "   split0_test_f1_weighted  split1_test_f1_weighted  split2_test_f1_weighted  \\\n",
       "0                 0.657743                 0.689646                 0.658725   \n",
       "1                 0.691427                 0.746788                 0.718728   \n",
       "2                 0.688419                 0.752780                 0.721704   \n",
       "3                 0.619923                 0.628129                 0.593432   \n",
       "4                 0.669500                 0.734118                 0.671878   \n",
       "5                 0.644037                 0.740262                 0.674924   \n",
       "\n",
       "   split3_test_f1_weighted  split4_test_f1_weighted  mean_test_f1_weighted  \\\n",
       "0                 0.637703                 0.672345               0.663239   \n",
       "1                 0.686490                 0.695931               0.707883   \n",
       "2                 0.676773                 0.711349               0.710211   \n",
       "3                 0.586191                 0.623787               0.610305   \n",
       "4                 0.636999                 0.672500               0.677022   \n",
       "5                 0.636603                 0.678798               0.674927   \n",
       "\n",
       "   std_test_f1_weighted  rank_test_f1_weighted  \n",
       "0              0.017214                      5  \n",
       "1              0.022375                      2  \n",
       "2              0.026603                      1  \n",
       "3              0.017070                      6  \n",
       "4              0.031507                      3  \n",
       "5              0.036631                      4  \n",
       "\n",
       "[6 rows x 38 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(search.cv_results_) #Model Comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#function for confusion matrix heatmap\n",
    "def confusion_matrix_heatmap(cm, index):\n",
    "    cmdf = pd.DataFrame(cm, index = index, columns=index)\n",
    "    dims = (5, 5)\n",
    "    fig, ax = plt.subplots(figsize=dims)\n",
    "    sns.heatmap(cmdf, annot=True, cmap=\"coolwarm\", center=0)\n",
    "    ax.set_ylabel('Actual')    \n",
    "    ax.set_xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.715\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.73      0.71       193\n",
      "          1       0.74      0.70      0.72       207\n",
      "\n",
      "avg / total       0.72      0.71      0.72       400\n",
      "\n",
      "[[141  52]\n",
      " [ 62 145]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUIAAAFACAYAAADJZXWXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAGQVJREFUeJzt3Xm8VXW9//HX5zCDIpMgAhcRURIlhTITywEzxVQcKrlmZhQ/Sm3waurtdtOGazfNsrqVGBo2oOaA2KApDqAiOaSoqUmOxwEUHFJUpu/vj73RA55hn33WPvts1uv5eKwHe3/X2uv73Spv12cN3x0pJSQpz+qqPQBJqjaDUFLuGYSScs8glJR7BqGk3DMIJeWeQSgp9wxCSblnEErKvc7VHkBTrus/xkdeatReF59Y7SGoDbofND3K+dwfu+xQ1t/Zg1Y/UlZ/WfKIUFLuGYSScs8glJR7BqGk3OuwF0sk1ZboUvVrHmXziFBS7hmEknLP0lhSJuo6WxpLUs0yCCXlnkEoKRPRpa6spcX9RlwYEcsi4oFG1p0cESkiBhTfR0T8OCKWRMTiiBhXytgNQkkd3a+AAzZujIhhwEeApxo0HwiMKi7TgJ+X0oFBKKlDSynNB1Y0suqHwNeAhpM9HApcnAruAPpExOCW+vCqsaRMtOdV44g4BHgmpXRfxAb9DgGebvC+vtj2XHP7MwglVVVETKNQxq43I6U0o5ntewJfB/ZvbHUjbS1OD2YQSqqqYug1GXyNGAmMANYfDQ4F7omI3SgcAQ5rsO1Q4NmWdmgQSspEez1rnFK6Hxj4dr8RTwDvSym9GBFzgRMi4hLgA8ArKaVmy2LwYomkDi4iZgMLgR0ioj4ipjaz+Z+Ax4AlwAXAF0vpwyNCSR1aSmlKC+u3afA6Ace3tg+PCCXlnkeEkjLhpAuSVMMMQkm5Z2ksKRNO1S9JNcwglJR7lsaSMuFVY0mqYQahpNyzNJaUiehkaSxJNcsglJR7BqGk3PMcoaRM1HmOUJJql0EoKfcMQkm55zlCSZmIOs8RSlLNMggl5Z6lsaRMRKfaPa6q3ZFLUkYMQkm5Z2ksKRM+WSJJNcwglJR7lsaSMuEN1ZJUwwxCSblnaSwpE141lqQaZhBKyj2DUFLueY5QUib8XWNJqmEGoaTcszSWlImoq93jqtoduSRlxCCUlHuWxpIy4aQLklTDDEJJuWcQSso9zxGWaMyPv82W++/FqhdXcPuek5vcrveuO7H7db/jvqkns/Sav7Spzy59tmDszHPoMWwIbzz9DPd99j9Y88qrDD7yIEZ8aSoAa15fyUMnf5t/PfhIm/pS0w789kx6dutCp7o6OtUFs086mnPnzueWvz9Gl06dGNp/C741ZX969+he7aFWlbPP5MCzs+dw9yf+X/Mb1dWx/TdP4sUbb2vVvvtOeD87/fS772of8eXPsWL+Im7dbRIr5i9i2698DoA3nnyGvx78GW7/8OE8ds4v2PGHZ7SqP7XeL7/4cS47+VPMPuloAHbfYThXnPJpLj/lGIZv2ZeZN9xZ5RGqLSoWhBExOiJOjYgfR8R5xdfvqVR/lfbSwrtZ/dIrzW4z/PNHs/Sa61n14ooN2rc54Th2v+FS9ph/JSNPPb7kPgdO2odnLpkDwDOXzGHgpH0BePnOe1nzyquF13ctpvvWg1rzVZSBPXYYTufiD5qPHT6YZa+8VuURqS0qEoQRcSpwCRDAX4E7i69nR8Rpleiz2roNHsjAgyby9EWXbtDef+896LntcO7Y75PcvtcR9H7vjvT94PiS9tl1y/6sWvoiAKuWvkjXAf3etc3QTx3OizcsaPsXUNMCpp9/JUed+1suX7j4Xavn/PUBJozepv3H1cFEXZS1dASVOkc4FRiTUlrdsDEizgUeBL7X2IciYhowDeBLPQczqXvfCg0ve6O/exr/+Na5sG7dBu3999mDAfvswQdvvgKAzr160nPb4by08G4+8JfZ1HXtSudePenSd4u3t/nHmeey/KaWy+t+e+7GkE8dzl8nHZP9F9LbZp34SQZusRnL/7WS6b+4ghED+zF+5FAALrh+EZ3q6jho/Ogqj1JtUakgXAdsDTy5Ufvg4rpGpZRmADMArus/JlVobBXRe5cxvPeCcwDo0q8vA/b7EGntGiKCx350AfWzfv+uzyzafwpQOEc4ZMpkHjjh6xusX/XCcroOGlA4Ghw0YIOSe7Mdt2fMj87k7k9Ob7FkV9sM3GIzAPpv3pN9d96OB556nvEjhzL3zgeZ//fHmfGFI4joGEc2Kk+lzhF+BZgXEX+OiBnF5VpgHvDlCvVZVQvGfZT5u+7P/F33Z+k1f+GhU77Dsj/dyIs33saQow+nU6+eQKGEbqzEbcyyP9/EkKMKV6iHHDWZZX+6CYDuQwaz66zzuP8Lp7Pynxv/v0ZZWvnWal5/c9Xbrxf+40m222oAtz30BBfdeBfnTT2EHl27VHmUm7aIuDAilkXEAw3azo6IhyNicURcFRF9Gqw7PSKWRMQjEfHRUvqoyBFhSunaiNge2A0YQuH8YD1wZ0ppbSX6rLSxM86m34T306V/H/a6fx5Lvvd/RJfCP776X13W5OeW33w7vbbflg9c+1sA1r6+ksXTT4ONLqg05vHzfsl7LzyXIUcfzpvPPMd9x50EwMhTptOl3xa85+xvAJDWruGOiZ9s61dUI1a89jpfvfAaANasW8ekcaOZ8J5t+Nh3L2TV2rVM/8WVAOw8fCu+8fH9qjnUqqvg7DO/An4KXNyg7Xrg9JTSmoj4X+B04NSI2BE4ChhDoSq9ISK2byl3IqWOWYHWWmmsd+x18YnVHoLaoPtB08uq8xdP2rusv7Nj/3Rzi/1FxDbAH1JKOzWy7jDgyJTS0RFxOkBK6aziuuuAM1JKC5vbv/cRSqqqiJgWEXc1WKa1chefBf5cfD0EeLrBuvpiW7N8skRSJsq9FabhRdJW9xnxdWAN8Nv1TY110dJ+DEJJNSkijgU+BkxM75zjqweGNdhsKPBsS/uyNJZUcyLiAOBU4JCU0soGq+YCR0VEt4gYAYyi8FBHszwilJSJSk26EBGzgb2BARFRD3yTwlXibsD1xXs470gpTU8pPRgRlwF/p1AyH1/KnSoGoaQOLaU0pZHmmc1s/13g3bOYNMPSWFLueUQoKRMdZQKFcnhEKCn3DEJJuWdpLCkTFXzWuOJqd+SSlBGDUFLuWRpLyoRXjSWphhmEknLPIJSUe54jlJQJzxFKUg0zCCXlnqWxpExYGktSDTMIJeWeQSgp9zxHKCkTzj4jSTXMIJSUe5bGkjJRqZ/zbA8eEUrKPYNQUu5ZGkvKhE+WSFINMwgl5Z5BKCn3PEcoKRM+WSJJNcwglJR7lsaSMuHtM5JUwwxCSblnaSwpE5bGklTDDEJJuWdpLCkT3lAtSTXMIJSUe5bGkjLhVWNJqmEGoaTcMwgl5V6T5wgj4hogNbU+pXRIRUYkqSbV8u0zzV0sOafdRiFJVdRkEKaUbmnPgUhStbR4+0xEjALOAnYEuq9vTyltW8FxSVK7KaWovwj4ObAG2Ae4GPh1JQclqQZFlLd0AKUEYY+U0jwgUkpPppTOAPat7LAkqSAiLoyIZRHxQIO2fhFxfUQ8Wvyzb7E9IuLHEbEkIhZHxLhS+iglCN+MiDrg0Yg4ISIOAwaW9Y0kqfV+BRywUdtpwLyU0ihgXvE9wIHAqOIyjUI126JSgvArQE/gS8B44Bjg2FJ2Lik/oi7KWlqSUpoPrNio+VBgVvH1LGByg/aLU8EdQJ+IGNxSHy1eLEkp3Vl8+RpwXIujlqTKG5RSeg4gpfRcRKyvUocATzfYrr7Y9lxzOyvlqvFNNHJjdUrJ84SS2iwiplEoY9ebkVKaUe7uGmlr8sGQ9UqZfebkBq+7A0dQuIIsSW8r98mSYui1NviWRsTg4tHgYGBZsb0eGNZgu6HAsy3trMWRp5TubrDcllI6CfhAKwctSVmayzvXKo4Frm7Q/uni1ePdgVfWl9DNKaU07tfgbR2FCyZbtWrIZdjrZ16PqVXzJv+w2kNQGxy0enq1h7CBiJgN7A0MiIh64JvA94DLImIq8BTw8eLmfwImAUuAlZR4XaOU0vhuCjV2UCiJHwemlvwtJOVCpSZmTSlNaWLVxEa2TcDxre2jlCB8T0rpzYYNEdGttR1JUkdVytnN2xtpW5j1QCSpWpqbj3ArCvff9IiIXXnnsnRvCjdYS9ImobnS+KPAZyhcfv4B7wThq8B/VnZYkmrNJjkxa0ppFjArIo5IKV3RjmOSpHZVSoSPj4g+699ERN+I+E4FxyRJ7aqUIDwwpfTy+jcppZco3KcjSW+r1KQL7aGUIOzU8HaZiOgBePuMpE1GKfcR/gaYFxEXFd8fxzvT30hSzStlGq7vR8RiYD8KV46vBYZXemCSaktHKXPLUer17ueBdRRmnpkIPFSxEUlSO2vuhurtgaOAKcBy4FIKv1uyTzuNTZLaRXOl8cPAAuDglNISgIj4aruMSlLtqeEbqpsb+REUSuKbIuKCiJhI47O/SlJNazIIU0pXpZQ+CYwGbga+CgyKiJ9HxP7tND5JqrhSZqh+PaX025TSxyg8d3wv7/x0niQBEBFlLR1Bq4r6lNKKlNL5/nCTpE1J7Z7dlKSMGISScq+UR+wkqUW1PB9h7Y5ckjJiEErKPYNQUu55jlBSJvIw+4wkbbIMQkm5Z2ksKRvePiNJtcsglJR7lsaSMuFVY0mqYQahpNyzNJaUiYjaPa6q3ZFLUkYMQkm5Z2ksKRteNZak2mUQSso9g1BS7nmOUFIm/M0SSaphBqGk3LM0lpQJJ12QpBpmEErKPUtjSdlw0gVJql0GoaTcszSWlAmvGktSDTMIJXV4EfHViHgwIh6IiNkR0T0iRkTEooh4NCIujYiu5e7fIJTUoUXEEOBLwPtSSjsBnYCjgP8FfphSGgW8BEwttw+DUFI26urKW0rTGegREZ2BnsBzwL7A5cX1s4DJZQ+93A9KUhYiYlpE3NVgmdZwfUrpGeAc4CkKAfgKcDfwckppTXGzemBIuWPwqrGkqkopzQBmNLU+IvoChwIjgJeB3wMHNrarcsfgEaGkjm4/4PGU0gsppdXAlcAeQJ9iqQwwFHi23A4MQkmZiIiylhI8BeweET2j8IGJwN+Bm4Aji9scC1xd7tgNQkkdWkppEYWLIvcA91PIrRnAqcBJEbEE6A/MLLcPzxFK6vBSSt8EvrlR82PAblns3yCUlA1/s0SSapdBKCn3LI0lZaKWZ58xCNvBq2+8xZlX38qSZS8RwJmTP8S8h57klkeeokunOob26823Jn+I3j26VXuom6SxF/wPAyftzaply5m/68FNbrfF+3Zmwq2Xcs+/f5Xnr7yuTX126bsFu/7uh/QcPoSVTz7DPVO+wpqXX2XrKQcz8pTPA7D2tde5/4Qz+NfiR9rUl9rO0rgdfP/PdzBh1FCu/tKR/P6LhzFiyz7sPnJrrjj+cC4//nCG9+/NzAX3VXuYm6z6WVfy1499rvmN6uoY/T8n88Jfbm3Vvvt9eDfGzjzrXe0jvzaN5Tcu5OYdP8ryGxey3dcKT4298UQ9C/f9FAvGHcKj3/05O//8263qT5VhEFbYa2+u4u4nnuewcdsD0KVzJ3r36MYe2w2lc6fCP/6xQwey7NWV1RzmJm3FrXexesUrzW6zzQnH8PxV1/HWC8s3aN/2pKlMWHg5H7pnLqP++8SS+xx08ETqfz0HgPpfz2HQIfsB8NLCv7Hm5VcLrxfdS48hW7Xmq3RsUVfe0gG0+ygi4rj27rOa6l/6F317dee/r1rAJ352FWfMWcDKVas32GbOPf9gwqihVRqhum09kK0O3Y8nz79kg/YB+02g16jh3PbBI1kw/lC2GDeGfnu+r7R9DurPW8+/AMBbz79At4H93rXNvx13JMuum9/2L6A2q0Ycn9nUioazUMy8YVF7jqli1q5bx8PPLefj7x/NZV88jB5dO3PhgsVvr7/glnvp1KmOg8aOrOIo823MD77Ow/95Dqxbt0H7lh+ZwID9JrDnXXPY886r2GyHbek1ahsA9rjtMva8aw5jz/8Ogz62b2Gbu+Yw4CN7ltRn/70+wLDjjuTh08/J+uuoDBW5WBIRi5taBQxq6nMNZ6F489Lvlz2TREcyqHcvBvXuxdhhAwH4yI4juLB4PnDu3x5l/iNPMeMzk0p95lIVsMX4ndj1N+cC0HVAXwYesBdpzRqI4J/fn8FTF1z6rs/cPuETQOEc4dBjD2Px1NM3WP/W0uV022rLwtHgVlvy1rIVb6/bfOcd2Pn873DnwZ9n9YqXK/jN2plXjd9lEPBRCrPGNhTA7RXqs0MasHlPBvXuxRMvvsw2A/qw6LFn2XZgX257tJ6Lbl3MzM9OokdXL95X003bT3z79diZZ7HsjzezdO481q58k+3P/DLP/O4a1r6+km5bDyStXsOqF1Y0s7eCpX+4kaHHTOafZ1/A0GMms/SaeQB0HzaY8Zf9hPuO+xqvP/pEpb6SWqlSfwP/AGyWUrp34xURcXOF+uywTjvog5x++S2sXruWoX0351uHfZh/P/9qVq1Zx/RZ1wKw89CBfOOQCVUe6aZpl1//gP577UbXAX3Z9/FbePRbPyG6FP7Tf2rGJU1+7sUbbmOz94xkj1sL26x9bSX3HntKSUH4z+/PYNzsHzHsuCN54+nnuOeoLwMw6r+Op2v/Poz5SeGx2bRmLbftfkRbv6LaKFLqmBXoplIa59G8T5U9CYg6gINWP1JWjfvaL04v6+/sZtPPqnpNbU0mKRPRQW6FKUftjlySMmIQSso9S2NJ2ajh22c8IpSUewahpNyzNJaUiXCqfkmqXQahpNwzCCXlnucIJWWjhmdQ8ohQUu4ZhJJyz9JYUja8fUaSapdBKCn3DEJJuec5QknZ8PYZSapdBqGk3LM0lpQJZ5+RpBpmEErKPUtjSdnw5zwlqXYZhJJyz9JYUjb8OU9Jql0GoaTcszSWlInwqrEk1S6DUFLuGYSScs9zhJKy4e0zklS7DEJJuWcQSspG1JW3lLLriD4RcXlEPBwRD0XEByOiX0RcHxGPFv/sW+7QDUJJteA84NqU0mjgvcBDwGnAvJTSKGBe8X1ZDEJJHVpE9AY+DMwESCmtSim9DBwKzCpuNguYXG4fBqGkbESUtUTEtIi4q8EybaM9bwu8AFwUEX+LiF9GRC9gUErpOYDinwPLHbq3z0iqqpTSDGBGM5t0BsYBJ6aUFkXEebShDG6MR4SSOrp6oD6ltKj4/nIKwbg0IgYDFP9cVm4HBqGkDi2l9DzwdETsUGyaCPwdmAscW2w7Fri63D4sjSVlo7I/53ki8NuI6Ao8BhxH4UDusoiYCjwFfLzcnRuEkjq8lNK9wPsaWTUxi/1bGkvKPY8IJWXDiVklqXYZhJJyz9JYUjacj1CSapdBKCn3DEJJuec5QknZ8PYZSapdBqGk3LM0lpSN8PYZSapZBqGk3LM0lpSNys5HWFG1O3JJyohBKCn3LI0lZaOGrxpHSqnaY8iliJhW/BlD1SD//W1aLI2rZ+MfsVZt8d/fJsQglJR7BqGk3DMIq8fzS7XNf3+bEC+WSMo9jwgl5Z5BKCn3DMIqiIgDIuKRiFgSEadVezwqXURcGBHLIuKBao9F2TEI21lEdAL+DzgQ2BGYEhE7VndUaoVfAQdUexDKlkHY/nYDlqSUHksprQIuAQ6t8phUopTSfGBFtcehbBmE7W8I8HSD9/XFNklVYhC2v8aeTPceJqmKDML2Vw8Ma/B+KPBslcYiCYOwGu4ERkXEiIjoChwFzK3ymKRcMwjbWUppDXACcB3wEHBZSunB6o5KpYqI2cBCYIeIqI+IqdUek9rOR+wk5Z5HhJJyzyCUlHsGoaTcMwgl5Z5BKCn3DMIci4i1EXFvRDwQEb+PiJ5t2NfeEfGH4utDmptVJyL6RMQXy+jjjIg4udwxSk0xCPPtjZTSLimlnYBVwPSGK6Og1f+NpJTmppS+18wmfYBWB6FUKQah1lsAbBcR20TEQxHxM+AeYFhE7B8RCyPinuKR42bw9ryKD0fErcDh63cUEZ+JiJ8WXw+KiKsi4r7isgfwPWBk8Wj07OJ2p0TEnRGxOCLObLCvrxfnbrwB2KHd/mkoVwxCERGdKcyPeH+xaQfg4pTSrsDrwH8B+6WUxgF3ASdFRHfgAuBg4EPAVk3s/sfALSml9wLjgAeB04B/Fo9GT4mI/YFRFKYo2wUYHxEfjojxFB5B3JVC0L4/468uAdC52gNQVfWIiHuLrxcAM4GtgSdTSncU23enMIHsbREB0JXCI2ajgcdTSo8CRMRvaPxHz/cFPg2QUloLvBIRfTfaZv/i8rfi+80oBOPmwFUppZXFPnwmWxVhEObbGymlXRo2FMPu9YZNwPUppSkbbbcL2U0fFsBZKaXzN+rjKxn2ITXJ0lgtuQOYEBHbAUREz4jYHngYGBERI4vbTWni8/OALxQ/2ykiegP/onC0t951wGcbnHscEhEDgfnAYRHRIyI2p1CGS5kzCNWslNILwGeA2RGxmEIwjk4pvUmhFP5j8WLJk03s4svAPhFxP3A3MCaltJxCqf1ARJydUvoL8DtgYXG7y4HNU0r3AJcC9wJXUCjfpcw5+4yk3POIUFLuGYSScs8glJR7BqGk3DMIJeWeQSgp9wxCSbn3/wF+Kuu64d5YsQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "#print(np.unique(featureset.iloc[:400,-1]))\n",
    "\n",
    "#print(x_test)\n",
    "predictions = search.predict(x_test)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "confusion_matrix_heatmap(confusion_matrix(y_test,predictions), search.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs = search.predict_proba(x_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "#Function for receiver operating charecteristics\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your question: where is california?\n",
      "is california \n",
      "455\n"
     ]
    }
   ],
   "source": [
    "import ftfy         #Preprocessing data\n",
    "import re\n",
    "hashtag_re = re.compile(r\"#\\w+\")\n",
    "mention_re = re.compile(r\"@\\w+\")\n",
    "url_re = re.compile(r\"(?:https?://)?(?:[-\\w]+\\.)+[a-zA-Z]{2,9}[-\\w/#~:;.?+=&%@~]*\")  #regex for url\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    p_text = hashtag_re.sub(\"\",text)\n",
    "    p_text = mention_re.sub(\"\",p_text)\n",
    "    p_text = url_re.sub(\"\",p_text)                 \n",
    "    p_text = ftfy.fix_text(p_text)\n",
    "    return p_text\n",
    "\n",
    "import wikipedia #wikipedia api to retrieve docs or pages from server\n",
    "from collections import Counter\n",
    "query = input('Please enter your question: ').lower() #user query Enter a question\n",
    "my_doc=nlp(query)\n",
    "token_list = []    #Creating  token list\n",
    "for token in my_doc:\n",
    "    token_list.append(token.text)\n",
    "sen=\"\"\n",
    "for word in token_list:\n",
    "    if word.lower() not in question_words:   #reformulation of query like before\n",
    "        sen=sen+word+\" \" \n",
    "query=sen\n",
    "print(query)\n",
    "pages=wikipedia.search(query, results=5)  #number of pages returned for user query here is 5, but can be changed accordingly\n",
    "text=''\n",
    "for page in pages:\n",
    "    text=text+wikipedia.page(page).content\n",
    "preprocessed_text=preprocess(text)\n",
    "preprocessed_text=preprocessed_text.split(\"== References ==\")[0] #removing garbage text after reference section\n",
    "sent_tokens=nltk.sent_tokenize(preprocessed_text.lower())\n",
    "\n",
    "print(len(sent_tokens))\n",
    "bc = BertClient(check_length=False)   #bert client object please make sure the server is On {MUST}\n",
    "for token in my_doc:\n",
    "    token_list.append(token.text) \n",
    "\n",
    "q_vec=bc.encode(sent_tokens)\n",
    "query_vec = bc.encode([query])[0]\n",
    "bert_score = np.sum(query_vec * q_vec, axis=1) / np.linalg.norm(q_vec, axis=1)   #cosine score from bert sentence matrix\n",
    "tfidf_cos1=[]\n",
    "lav_dis1=[]\n",
    "jac_sim1=[]\n",
    "mscore1=[]\n",
    "ham_dis1=[]  #**All variables and functions used here are similar like feature extraction from wikiqa for featrure extraction**\n",
    "jwscore1=[]\n",
    "sdscore1=[]\n",
    "roscore1=[]\n",
    "escore1=[]\n",
    "wmd1=[]\n",
    "ner_feature1=[]\n",
    "common_keywords1=[]\n",
    "count_noun_chunks1=[]\n",
    "for context in sent_tokens:\n",
    "\n",
    "    tfidf_cos=tfidf(query,context)\n",
    "    tfidf_cos1.append(tfidf_cos)\n",
    "    q_ner=list()\n",
    "    doc1=nlp(sen)\n",
    "    for ent in doc1.ents:\n",
    "        q_ner.append(ent.text)\n",
    "    \n",
    "    doc2=nlp(context)\n",
    "    ner_count=0\n",
    "    for ent in doc2.ents:\n",
    "        if ent.text in q_ner:\n",
    "            ner_count=ner_count+1\n",
    "    ner_feature1.append(ner_count)\n",
    "    keycount=0\n",
    "    rootq = [token for token in doc1 if token.head == token][0]\n",
    "    keywordsq=[t.text for t in rootq.subtree if t.pos_==\"NOUN\" or t.pos_==\"PROPN\" or t.pos_==\"VERB\" or t.pos_==\"NUM\"]\n",
    "    rootc = [token for token in doc2 if token.head == token][0]\n",
    "    keywordsc=[t.text for t in rootc.subtree if t.pos_==\"NOUN\" or t.pos_==\"PROPN\" or t.pos_==\"VERB\" or t.pos_==\"NUM\"]\n",
    "    for key in keywordsq:\n",
    "        if key in keywordsc:\n",
    "            keycount=keycount+1\n",
    "    common_keywords1.append(keycount)\n",
    "    ner_feature1.append(ner_count)\n",
    "    q_noun_chunks=list(doc1.noun_chunks)\n",
    "    c_noun_chunks=list(doc2.noun_chunks)\n",
    "    count_chunks=0\n",
    "    for chunk in c_noun_chunks:\n",
    "        if chunk in q_noun_chunks:\n",
    "            count_chunks=count_chunks+1\n",
    "    count_noun_chunks1.append(count_chunks)\n",
    "    #cosine.append(0)\n",
    "    temp=glove_300.wmdistance(ques,context)\n",
    "    if np.isnan(temp)==False:\n",
    "        temp=0.0\n",
    "    wmd1.append(temp)\n",
    "    lav_dis1.append(nltk.edit_distance(ques,context))\n",
    "    jac_sim1.append(jaccard_similarity(ques.split(),context.split()))\n",
    "    #jscore=jaccard_similarity(question_vec,c_vec.flatten())\n",
    "    mscore1.append(masi_distance(set(ques.split()),set(context.split())))\n",
    "    ham_dis1.append(textdistance.hamming(ques, context))\n",
    "    jwscore1.append(textdistance.jaro_winkler(ques,context))\n",
    "    #Sorensen-Dice\n",
    "    sdscore1.append(textdistance.sorensen(ques.split() , context.split()))\n",
    "    #Ratcliff-Obershelp similarity\n",
    "    roscore1.append(textdistance.ratcliff_obershelp(ques,context))\n",
    "    escore1.append(textdistance.entropy_ncd(ques,context))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_featureset = pd.DataFrame(list(zip(sent_tokens,bert_score,lav_dis1,jac_sim1,mscore1,ham_dis1,jwscore1,sdscore1,roscore1,escore1,ner_feature1,count_noun_chunks1,common_keywords1,wmd1,tfidf_cos1)), \n",
    "               columns =[\"Possible Answers\",\"Cosine_Similarity\",\"Levenshtein_distance\",\"Jaccard_similarity\",\"Masi_distance\",\"Hamming_dustance\",\"JaroWinkler_socre\",\"Sorensen-Dice\",\"Ratcliff-Obershelp_similarity\",\"NCD_entropy\",\"Common_NER\",\"Common_noun_chunks\",\"Common_Keywords\",\"Word-Movers Distance\",\"Tfidf_Cosine_Similarity\"])\n",
    "#print(test_featureset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0\n",
      " 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
      " 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0\n",
      " 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0\n",
      " 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "predictions = search.predict(test_featureset.iloc[:,1:])\n",
    "print(predictions) #predicted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31 46 63 87 58]\n",
      "[\"a majority of california's cities are located in either the san francisco bay area or the sacramento metropolitan area in northern california; or the los angeles area, the riverside-san bernardino-inland empire, or the san diego metropolitan area in southern california.\", '=== national origins ===\\naccording to the united states census bureau in 2018 the population self-identifies as (alone or in combination):\\n72.1% white (including hispanic whites)\\n15.3% asian\\n6.5% black or african american\\n1.6% native american and alaska native\\n0.5% native hawaiian or pacific islander\\n3.9% two or more racesby ethnicity, in 2018 the population was 60.7% non-hispanic (of any race) and 39.3% hispanic or latino (of any race).', \"while proposition 30 also enacted a minimum state sales tax of 7.5%, this sales tax increase was not extended by proposition 55 and reverted to a previous minimum state sales tax rate of 7.25% in 2017. local governments can and do levy additional sales taxes in addition to this minimum  real property is taxable annually; the ad valorem tax is based on the property's fair market value at the time of purchase or the value of new construction.\", 'the california national party and the california freedom coalition both advocate for californian independence along the lines of progressivism and civic nationalism.', 'there are ten general uc campuses, and a number of specialized campuses in the uc system, as the uc san francisco, which is entirely dedicated to graduate education in health care, and is home to the ucsf medical center, the highest ranked hospital in california.']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "predicted_answers=test_featureset.iloc[np.where(predictions==1)[0].flatten()] #using indices of predicted answers\n",
    "#print(predicted_answers)\n",
    "normalized_scores=MinMaxScaler().fit_transform(predicted_answers.iloc[:,1:]) #|MInMAx scalar used here for feature data normalisation\n",
    "#print(np.shape(normalized_scores)[1])\n",
    "sum=0\n",
    "#print(predicted_answers)\n",
    "for i in range(0,np.shape(normalized_scores)[1]):      #sum of all normalized scores\n",
    "    sum=sum+normalized_scores[:,i]\n",
    "#print(sum)\n",
    "#common NER and common noun chunks will have a slight priority\n",
    "sum=sum-predicted_answers.iloc[:,10].values-predicted_answers.iloc[:,11].values\n",
    "#print(sum)\n",
    "answer_index=sum.argsort()[:5]\n",
    "print(answer_index)\n",
    "best_answer=predicted_answers.iloc[answer_index,0]\n",
    "print(best_answer.values.tolist())  #Top 5 answers returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
