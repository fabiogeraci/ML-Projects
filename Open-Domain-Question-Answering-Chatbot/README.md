The quest for knowledge is deeply human, and so it is not surprising that practically as soon as there were computers we were asking them questions. Using Wikipedia articles as the knowledge source causes the task of question answering(QA) to combine the challenges of both large-scale open-domain QA and of machine comprehension of text. This paper considers the problem of answering factoid questions in an open-domain setting usingWikipediaastheuniqueknowledgesource: the answer to any factoid question is a text span in a Wikipedia article. Unlike most QA or reading comprehension models, which are best described as rerankers or extractors since they assume as input relatively small amounts of text (an article, top k sentences or passages, etc.)[2], this system integrates state of the art word embeddings viz. BERT[6], GLOVE-300[5] to not only produce ranking matrices for QA pair, but also utilizes the matrices on WikiQA[4] dataset to train our optimalmodelandﬁnallychoosethebestpossible answer based on the proprietary ranking algorithm applied here on a large corpus of several wikipedia pages. 

Large-scale QA systems like IBM’s DeepQA (Ferrucci et al., 2010)[7] rely on multiple sources to answer: besides Wikipedia, it is also paired with KBs(knowledgeBase),dictionaries,andevennews articles, books, etc. As a result, such systems heavily rely on information redundancy among the sources to answer correctly. Having a single knowledge source forces the model to be very precise while searching for the optimal answer for the question based on linguistic learning algorithms as the evidence might appear only once. As NLP researchers became increasingly interested in QA, they placed greater emphasis on the later stages of the pipeline to emphasize various aspects of linguistic analysis[2]. This challenge thus encourages research in the ability of a machine to read, a key motivationforthemachinecomprehensionsubﬁeld and the creation of datasets such as SQuAD (Rajpurkaretal.,2016)[8],CNN/DailyMail(Hermann et al., 2015)[9] and CBT (Hill et al., 2016). 


However, those machine comprehension resources typically assume that a short piece of relevant text is already identiﬁed and given to the model, which is not realistic for building an opendomainQAsystem. Insharpcontrast,methodsthat use KBs or information retrieval over documents have to employ search as an integral part of the solution. This project is focused on simultaneously maintaining the challenge of machine comprehension,whichrequiresthedeepunderstandingoftext, while keeping the realistic constraint of searching over a large open resource. BERT (Devlin et al., 2018)[6], the latest reﬁnement of a series of neural models that make heavy use of pre-training (Peters et al., 2018; Radford et al., 2018), has led to impressivegainsinmanynaturallanguageprocessing tasks;hasbeenimplementedinthisprojectforboth feature extraction and ranking metric. This project has extensively bulit and validated our QA model based on WikiQA[4](Yang et al., 2015) data-set andﬁnallythepredictedanswersbythemodel(corpus of several pages returned on search query from Wikipedia Server) have been ranked(proprietary ranking algorithm) to provide the user top 5 optimal answers for the query.
